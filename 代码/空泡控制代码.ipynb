{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7942baeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tiannananna\\venv\\lib\\site-packages\\ipykernel_launcher.py:377: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "matplotlib.use('agg')  #可以减少内存的使用\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from numpy import cos, sin\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "xuhao = 14\n",
    "\n",
    "###超参数调试区  \n",
    "#基本不用动的参数\n",
    "gamma = 0.99\n",
    "tau = 0.002  # 软更新参数\n",
    "buffer_size = 24000  # 40000\n",
    "minimal_size = 4000  # 1000\n",
    "batch_size = 128 # 128\n",
    "\n",
    "sigma = 0.01  # 高斯噪声标准差\n",
    "\n",
    "action_bound = 0.245 #动作范围\n",
    "noise = 1\n",
    "noise_1_bound = noise\n",
    "#需要调参区   暴力破解\n",
    "actor_lr = xuhao * 1e-4#学习率\n",
    "critic_lr = xuhao * 1e-3\n",
    "episodes = 4000  #回合数\n",
    "Tmax = 40  #仿真时间\n",
    "steps = 400  #步数\n",
    "state_limit_episode = minimal_size/steps  #神经网络开始学习需要的回合数\n",
    "\n",
    "def reward_function(error,x1dot,x1c,x1): #奖励函数\n",
    "    bound = 1\n",
    "    if x1 <x1c - bound and x1dot>0 :\n",
    "        a = - 20*np.sqrt((error/20)**2)  + 1    \n",
    "    elif x1> x1c+ bound and x1dot<0:\n",
    "        a = - 20*np.sqrt((error/20)**2)  + 1\n",
    "    elif x1c- bound <= x1 <=x1c + bound:\n",
    "        a =  10 *  np.absolute(1 - np.absolute(error)/bound)\n",
    "    else:\n",
    "        a =  - 20*np.sqrt((error/20)**2)\n",
    "\n",
    "    reward = a \n",
    "\n",
    "    return reward\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)  # 从指定的序列中，随机的截取指定长度的片段\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def soft_update(net, target_net):  # 采用软更新\n",
    "    for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim, fn=lambda x: (torch.tanh(x))):  # 最早是1\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_in, 64)\n",
    "        self.fc2 = nn.Linear(64, hidden_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc3 = nn.Linear(64, num_out)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        # print(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc22(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.fn(x)\n",
    "        return x\n",
    "\n",
    "actor = Net(3, 1, 64).to(\"cuda\")\n",
    "target_actor = Net(3, 1, 64).to(\"cuda\")\n",
    "critic_1 = Net(4,1,128,fn = lambda x:x).to(\"cuda\")\n",
    "target_critic_1 = Net(4,1,128,fn = lambda x:x).to(\"cuda\")\n",
    "critic_2 = Net(4,1,128,fn = lambda x:x).to(\"cuda\")\n",
    "target_critic_2 = Net(4,1,128,fn = lambda x:x).to(\"cuda\")\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic_1.load_state_dict(critic_1.state_dict())\n",
    "target_critic_2.load_state_dict(critic_2.state_dict())\n",
    "\n",
    "def update(tran_dict,episode):\n",
    "    states = torch.tensor(tran_dict['states'], dtype=torch.float).to(\"cuda\")\n",
    "    actions = torch.tensor(tran_dict['actions'], dtype=torch.float).view(-1, 1).to(\"cuda\")\n",
    "    next_states = torch.tensor(tran_dict['next_states'], dtype=torch.float).to(\"cuda\")\n",
    "    dones = torch.tensor(tran_dict['dones'], dtype=torch.float).view(-1, 1).to(\"cuda\")\n",
    "    rewards = torch.tensor(tran_dict['rewards'], dtype=torch.float).view(-1, 1).to(\"cuda\")\n",
    "    #下一时刻做出下一时刻动作的q值\n",
    "    global noise_1_bound\n",
    "    \n",
    "    next_q_values_min = None\n",
    "\n",
    "    #目标策略平滑，在目标策略的输出动作中加入噪声，以此来平滑q函数的估值，避免过拟合。\n",
    "    #在此处加入高斯噪音并clip截断\n",
    "    noise_1 = sigma * np.random.randn() * noise_1_bound  #标准正态分布\n",
    "    \n",
    "    #虽然要添加噪音来平滑，但是还是要逐渐减小为好\n",
    "    noise_1_bound = noise_1_bound * 0.99\n",
    "    \n",
    "    #clip将高斯噪音进行限幅\n",
    "    noise_1 = np.clip(noise_1,-sigma*2*noise_1_bound,sigma*2*noise_1_bound)\n",
    "    \n",
    "    #将加入噪音的动作进行限幅\n",
    "    target_action = torch.clamp(target_actor(next_states) + torch.tensor(noise_1,dtype=torch.float).to(\"cuda\"),-1,1)   #clamp限幅函数\n",
    "    #限制最终的target动作在-1到1之间//////\n",
    "\n",
    "    next_q_values_1 = target_critic_1(torch.cat([next_states, target_action], dim=1))\n",
    "    next_q_values_2 = target_critic_2(torch.cat([next_states, target_action], dim=1))\n",
    "    #此处也未参与反向传播\n",
    "    \n",
    "    #计算目标值时，利用二者间的较小值来进行计算\n",
    "    if next_q_values_1[0] > next_q_values_2[0]:\n",
    "        next_q_values_min = next_q_values_2\n",
    "    else:\n",
    "        next_q_values_min = next_q_values_1  \n",
    "\n",
    "    #这一时刻做出这一动作的目标q值  #也要分开不然报错\n",
    "    q_targets = rewards + gamma * next_q_values_min * (1 - dones)\n",
    "    #两个价值网络都进行损失函数的反向传播\n",
    "    #误差函数要分开写，不然会报错\n",
    "    #critic_1网络的更新\n",
    "    critic_loss_1 = torch.mean(F.mse_loss(critic_1(torch.cat([states, actions], dim=1)), q_targets))\n",
    "    critic_optimizer_1.zero_grad()\n",
    "    critic_loss_1.backward(retain_graph=True)  #要加上这个才行\n",
    "    critic_optimizer_1.step()\n",
    "    #critic_2网络的更新\n",
    "    critic_loss_2 = torch.mean(F.mse_loss(critic_2(torch.cat([states, actions], dim=1)), q_targets))\n",
    "    critic_optimizer_2.zero_grad()\n",
    "    critic_loss_2.backward()\n",
    "    critic_optimizer_2.step()\n",
    "    \n",
    "    #每隔d个回合再进行actor网络以及所有目标网络的更新\n",
    "    #%为求episode的余数，可以理解为每隔2个回合过呢更新一次\n",
    "    if episode % 2 == 0 :  #如果episode恰好可以被2整除，也就是偶数回合\n",
    "        #actor网络的损失函数\n",
    "        #ps：伪代码中只是利用critic_1网络来进行actor网络的学习，应该影响不大\n",
    "        actor_loss = -torch.mean(critic_1(torch.cat([states,actor(states)],dim=1)))\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()     \n",
    "        #软更新目标网络\n",
    "        soft_update(actor,target_actor)\n",
    "        soft_update(critic_1,target_critic_1)\n",
    "        soft_update(critic_2,target_critic_2)\n",
    "\n",
    "class env:  #模型环境\n",
    "    def __init__(self):\n",
    "        #self.V = 20\n",
    "        self.g = 9.8\n",
    "        self.h = 20\n",
    "        self.R = 2.5\n",
    "        self.Cx0 = 0.1\n",
    "        self.dt = Tmax/steps  #单位时间\n",
    "        self.omega_n = 3.66\n",
    "        self.a0 = -0.3379\n",
    "        self.a1 = 6.8762\n",
    "        self.a2 = 1.487\n",
    "        self.rhow = 998\n",
    "        self.x1c = 6\n",
    "        self.Dn = self.R*2  ##等效空化器直径\n",
    "        self.p0 = 101000  ##大气压强\n",
    "        self.pc = 3500    ##饱和蒸汽压 \n",
    "    def reset(self,episode):\n",
    "        self.t = 0\n",
    "        \n",
    "        self.V =  3 * np.sin(np.pi*self.t/5) + 27\n",
    "        #更新\n",
    "        self.x1 = 0\n",
    "        self.x2 = 0\n",
    "        \n",
    "        self.LcavReal = self.x1\n",
    "        self.LcavRealdot = self.x2        \n",
    "        \n",
    "        self.done = False\n",
    "#         #\n",
    "#         self.Fr = self.V/np.sqrt(self.g*self.h)  #定值\n",
    "#         self.Sigmac = self.Fr**(-4) + self.Fr**(-2) * (self.Fr**(-4) + 0.073/(4*self.Qin))**2  #变\n",
    "#         self.cd = self.Cx0 * (1 + self.Sigmac) #变\n",
    "#         self.LcavSteady = 2 * self.R * np.sqrt(self.cd * np.log(1/self.Sigmac))/self.Sigmac #变        \n",
    "        \n",
    "        self.error = self.x1c - self.x1\n",
    "        #self.x2,self.Sigmac,self.cd,self.LcavSteady,self.x1,self.error\n",
    "        return [self.x1,self.error,self.V]\n",
    "\n",
    "    def step(self, action ,episode):\n",
    "        \n",
    "        self.Qin= action[0]\n",
    "        self.cq = self.Qin / (self.V*self.Dn**2)\n",
    "        self.Fr = self.V/np.sqrt(self.g*self.h)  #定值\n",
    "        self.k = 16/(0.015*np.pi*self.Cx0)\n",
    "        self.Sigmav = (self.g*self.h*self.rhow+self.p0-self.pc)/(0.5*self.rhow*self.V**2)\n",
    "        self.Sigmac = 2*self.Sigmav/(1+np.sqrt(1+self.k*self.Sigmav*self.cq))\n",
    "        self.cd = self.Cx0 * (1 + self.Sigmac) #变\n",
    "        self.LcavSteady = 2 * self.R * np.sqrt(self.cd * np.log(1/self.Sigmac))/self.Sigmac #变\n",
    "        \n",
    "        #四阶龙格库塔积分\n",
    "        kx = np.tanh(self.a1 * self.LcavRealdot) * self.a0 + self.a2\n",
    "        \n",
    "        x1dot = self.x2 \n",
    "        x2dot = -(self.omega_n)**2 * self.x1 - 2 * kx * self.omega_n * self.x2 + (self.omega_n)**2 * self.LcavSteady\n",
    "        k1 =   [self.dt*x1dot,self.dt*x2dot]\n",
    "        \n",
    "        x1dot = self.x2 + 0.5 * k1[1]\n",
    "        x2dot = -(self.omega_n)**2 * (self.x1 + 0.5 * k1[0]) -2 * kx * self.omega_n * (self.x2 + 0.5 * k1[1]) + (self.omega_n)**2 * self.LcavSteady\n",
    "        k2 =  [self.dt*x1dot,self.dt*x2dot]\n",
    "        \n",
    "        x1dot = self.x2 + 0.5 * k2[1]\n",
    "        x2dot = -(self.omega_n)**2 * (self.x1 + 0.5 * k2[0]) -2 * kx * self.omega_n * (self.x2 + 0.5 * k2[1]) + (self.omega_n)**2 * self.LcavSteady\n",
    "        k3 = [self.dt*x1dot,self.dt*x2dot]\n",
    "        \n",
    "        x1dot = self.x2 + k3[1]\n",
    "        x2dot = -(self.omega_n)**2 * (self.x1 + k3[0]) -2 * kx * self.omega_n * (self.x2 +  k3[1]) + (self.omega_n)**2 * self.LcavSteady\n",
    "        k4 =[self.dt*x1dot,self.dt*x2dot]\n",
    "        \n",
    "        self.x1 = self.x1 + (k1[0] + 2* k2[0] + 2 * k3[0] + k4[0])/6\n",
    "        self.x2 = self.x2 + (k1[1] + 2* k2[1] + 2 * k3[1] + k4[1])/6\n",
    "        \n",
    "        self.LcavReal = self.x1\n",
    "        self.LcavRealdot = self.x2\n",
    "        \n",
    "        self.error = self.x1c - self.x1\n",
    "        \n",
    "        self.r = reward_function(self.error,self.x2,self.x1c,self.x1)\n",
    "        \n",
    "        self.t = self.t + self.dt\n",
    "        \n",
    "        self.V = 3 * np.sin(np.pi*self.t/5) + 27\n",
    "        \n",
    "        #判断是否超出状态空间\n",
    "        if self.x1 > 20 or self.x1 < -5:\n",
    "            \n",
    "            self.done = True  # 真的出界\n",
    "            #\n",
    "        return [self.x1,self.error,self.V], self.r, self.done\n",
    "\n",
    "gaochao = env()\n",
    "\n",
    "#存储所有回合的奖励\n",
    "return_list = []\n",
    "#存储每个回合的动作、奖励、动作\n",
    "state_buffer = []\n",
    "reward_buffer = []\n",
    "action_buffer = []\n",
    "#经验池\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "actor_optimizer = torch.optim.NAdam(actor.parameters(), lr=actor_lr)\n",
    "critic_optimizer_1 = torch.optim.NAdam(critic_1.parameters(), lr=critic_lr)\n",
    "critic_optimizer_2 = torch.optim.NAdam(critic_2.parameters(), lr=critic_lr)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    noise = noise * 0.993  #噪音随着回合数的增大而逐渐减小\n",
    "    episode_return = 0 #每回合开始时将回合奖励清零\n",
    "    state = gaochao.reset(episode)  #环境给出初始参数\n",
    "    done = False  #超出警告标志回归为未超出状态空间范围\n",
    "    #将上个回合的动作、奖励、动作清除\n",
    "    state_buffer.clear()\n",
    "    reward_buffer.clear()\n",
    "    action_buffer.clear()    \n",
    "#     print(episode)  #输出是第几个回合\n",
    "    for step in range(steps):\n",
    "#         #复制状态，将复制的状态经过归一化后输入到actor网络中，并最终存入经验池中\n",
    "#         print(state)\n",
    "        st = copy.deepcopy(state) \n",
    "#         st[0] = st[0] / 20\n",
    "#         st[1] = st[1] / 1\n",
    "#         st[2] = st[2] / 0.2\n",
    "#         st[3] = st[3] / 1.2\n",
    "        st[0] = st[0] / 20\n",
    "        st[1] = st[1] / 20\n",
    "        st[2] = st[2] / 30\n",
    "        #将归一化的状态量输入actor'网络中\n",
    "        s = torch.tensor(st,dtype = torch.float).to(\"cuda\") #更改格式只有tensor格式才能输入到神经网络中\n",
    "        action = actor(s)\n",
    "        \n",
    "        action = action.tolist() #转换为list格式\n",
    "        \n",
    "        ac = copy.deepcopy(action)  #复制动作并进行归一化以便存入经验池\n",
    "        \n",
    "        action[0] = 0.255 + action[0] * action_bound + sigma * np.random.randn() * noise #将输出值化为动作并添加噪音\n",
    "        \n",
    "        if action[0] > 0.5:\n",
    "            action[0] = 0.5\n",
    "        if action[0] < 0.01:\n",
    "            action[0] = 0.01\n",
    "            \n",
    "        action_buffer.append(action)  #存动作\n",
    "\n",
    "        #对环境输入动作，输出下一时刻的状态、奖励、标志\n",
    "        next_state, reward , done = gaochao.step(action,episode)\n",
    "        \n",
    "        if done == True : \n",
    "            break\n",
    "        \n",
    "        state_buffer.append(state)  #存状态\n",
    "        reward_buffer.append(reward) #存奖励\n",
    "        \n",
    "        ns = copy.deepcopy(next_state)  #复制下一时刻的状态并归一化\n",
    "#         ns[0] = ns[0] / 20\n",
    "#         ns[1] = ns[1] / 1\n",
    "#         ns[2] = ns[2] / 0.2\n",
    "#         ns[3] = ns[3] / 1.2\n",
    "        ns[0] = ns[0] / 20\n",
    "        ns[1] = ns[1] / 20\n",
    "        ns[2] = ns[2] / 30\n",
    "        replay_buffer.add(st,ac,reward,ns,done)  #将状态/动作/奖励/下一时刻的状态/是否越界存入经验池\n",
    "        \n",
    "        state = next_state #更新状态\n",
    "        episode_return += reward #加上这一步的奖励\n",
    "        \n",
    "        if replay_buffer.size() > minimal_size:  #如果经验池中的元组数超过神经网络开始学习所需要的元组数\n",
    "            b_s , b_a , b_r , b_ns , b_d = replay_buffer.sample(batch_size) #返还指定批量的元组\n",
    "            #设置字典方便学习\n",
    "            transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d}\n",
    "            update(transition_dict,episode)  #神经网络进行更新\n",
    "            \n",
    "            \n",
    "#     print(episode_return) #输出每回合的奖励\n",
    "#     print(\"--------------\") #分隔线有利于观察\n",
    "    \n",
    "    return_list.append(episode_return) #将本回合的奖励存入所有奖励的列表中\n",
    "    \n",
    "\n",
    "#    torch.save(actor.state_dict(),'D:/pjy/actor_net/{}.pth'.format(episode)) \n",
    "#存数据区\n",
    "  #  state_buffer = np.array(state_buffer) #需要先将列表转化为矩阵格式才能调取其中的\n",
    "    #存状态量的变化数据\n",
    "    #self.alpha, self.v2, self.fu, self.Wz,self.T,self.L\n",
    "#     data = pd.DataFrame(state_buffer[:,0])\n",
    "#     data.to_csv('D:/pjy/state_data/{}alpha.csv'.format(episode))\n",
    "#     data = pd.DataFrame(state_buffer[:,1])\n",
    "#     data.to_csv('D:/pjy/state_data/{}v2.csv'.format(episode))  \n",
    "#     data = pd.DataFrame(state_buffer[:,2])\n",
    "#     data.to_csv('D:/pjy/state_data/{}fu.csv'.format(episode))    \n",
    "#     data = pd.DataFrame(state_buffer[:,3])\n",
    "#     data.to_csv('D:/pjy/state_data/{}Wz.csv'.format(episode))    \n",
    "#     data = pd.DataFrame(state_buffer[:,4])\n",
    "#     data.to_csv('D:/pjy/state_data/{}T.csv'.format(episode))    \n",
    "#     data = pd.DataFrame(state_buffer[:,5])\n",
    "#     data.to_csv('D:/pjy/state_data/{}L.csv'.format(episode))    \n",
    "\n",
    "    #存动作量的变化数据\n",
    "#     data = pd.DataFrame(action_buffer)\n",
    "#     data.to_csv('D:/pjy/action_data/{}.csv'.format(episode))    \n",
    "    \n",
    "    \n",
    "    #画图区\n",
    "    state_buffer = np.array(state_buffer)\n",
    "    u = np.arange(0,400,0.01) #目标值的x轴数值\n",
    "    target = np.full(shape=(40000,1),fill_value = 6) #目标值的y轴数值  fill_value为目标值\n",
    "    \n",
    "    x1 = len(state_buffer)\n",
    "    x1 = np.arange(x1) #state的x轴数值\n",
    "    plt.figure() #新建一个画布\n",
    "    \n",
    "    plt.title('state %1.3f' % episode)  #标题\n",
    "    plt.plot(x1 , state_buffer[:,0]) #绘状态变化的图\n",
    "    plt.plot(u,target) #绘制目标值的直线\n",
    "    \n",
    "    state_buffer = state_buffer.tolist()\n",
    "#     plt.savefig(\"./pjy{}/png/{}.png\".format(xuhao,episode))  #将画好的图存到指定的位置上\n",
    "# #     plt.show()\n",
    "\n",
    "#     x2 = len(action_buffer)\n",
    "#     x2 = np.arange(x2)\n",
    "#     plt.figure()\n",
    "#     plt.title('action %1.3f' %episode)\n",
    "#     plt.plot(x2 , action_buffer)\n",
    "#     plt.savefig(\"./pjy{}/action_data/{}.png\".format(xuhao,episode))\n",
    "    \n",
    "#     plt.close(\"all\")  #将所有的画布删去防止占用内存\n",
    "\n",
    "    \n",
    "#  833  841  957   1165  1385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de73b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b14e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
